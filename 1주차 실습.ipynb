{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cdc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='í•œêµ­ì—ëŠ” 2023ë…„ ê¸°ì¤€ìœ¼ë¡œ 17ê°œì˜ ê´‘ì—­ìì¹˜ë‹¨ì²´ê°€ ìˆìœ¼ë©°, ì´ ì¤‘ 6ê°œëŠ” íŠ¹ë³„ì‹œì™€ ê´‘ì—­ì‹œì…ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ 11ê°œëŠ” ë„(é“)ì…ë‹ˆë‹¤. ê° ë„ì™€ ê´‘ì—­ì‹œ ì•„ë˜ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ ì‹œì™€ êµ°ì´ ìˆìŠµë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ í•œêµ­ì˜ ë„ì‹œ ìˆ˜ëŠ” ìˆ˜ë°± ê°œì— ë‹¬í•©ë‹ˆë‹¤. ì •í™•í•œ ìˆ˜ì¹˜ëŠ” í–‰ì • êµ¬ì—­ì˜ ë³€í™”ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' response_metadata={'token_usage': {'completion_tokens': 97, 'prompt_tokens': 16, 'total_tokens': 113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_0392822090', 'finish_reason': 'stop', 'logprobs': None} id='run-8fdc22b4-ee4b-464e-8cfc-a3e8d8f8f53f-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\") \n",
    "\n",
    "response = chat.invoke(\"í•œêµ­ì— ë„ì‹œê°€ ëª‡ê°œì¸ê°€ìš”??\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e888af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Response 1: í•œêµ­ê³¼ ëŒ€ë§Œì˜ ê±°ë¦¬ëŠ” ëŒ€ëµ 1,600í‚¬ë¡œë¯¸í„° ì •ë„ì…ë‹ˆë‹¤. ì„œìš¸ì—ì„œ íƒ€ì´ë² ì´ê¹Œì§€ì˜ ì§ì„  ê±°ë¦¬ë¡œ ê³„ì‚°í–ˆì„ ë•Œì˜ ëŒ€ëµì ì¸ ìˆ˜ì¹˜ì´ë©°, ì‹¤ì œ ë¹„í–‰ ê±°ë¦¬ë‚˜ í•­ë¡œì— ë”°ë¼ ë‹¤ì†Œ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ğŸŸ¢ 1. ì¼ë°˜ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "\n",
    "#ì§€ê¸ˆì€ ì„¸ì¤„ì´ì§€ë§Œ ì´ê²ƒë„ í•œì¤„ë¡œ ë°”ë€”ê²ƒì…ë‹ˆë‹¤..\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"{country_a}ì™€ {country_b}ì˜ ê±°ë¦¬ëŠ” ì–¼ë§ˆì¸ê°€ìš”??\"\n",
    ")\n",
    "\n",
    "formatted_prompt = template.format(country_a=\"í•œêµ­\", country_b=\"ëŒ€ë§Œ\")\n",
    "\n",
    "response1 = chat.invoke(formatted_prompt)\n",
    "\n",
    "print(\"ğŸŸ¢ Response 1:\", response1.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a4313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Response 2: ë©•ì‹œì½”ì™€ íƒœêµ­ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì•½ 14,000í‚¬ë¡œë¯¸í„° ì •ë„ì…ë‹ˆë‹¤. ì œ ì´ë¦„ì€ í‚¤ì›€ì¦ê¶Œì…ë‹ˆë‹¤! ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "# ğŸŸ¢ 2. ì—­í•  ê¸°ë°˜ ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"Ciao, mi chiamo {name}!\"),\n",
    "    (\"human\", \"What is the distance between {country_a} and {country_b}? Also, what is your name?\"),\n",
    "])\n",
    "\n",
    "chat_messages = chat_template.format_messages(\n",
    "    language=\"í•œêµ­ì–´\",\n",
    "    name=\"í‚¤ì›€ì¦ê¶Œ\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "response2 = chat.invoke(chat_messages)\n",
    "\n",
    "print(\"ğŸŸ¢ Response 2:\", response2.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dce2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì§€êµ¬', 'í™”ì„±', 'ëª©ì„±', 'ê¸ˆì„±', 'í† ì„±']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))  # ê° í•­ëª©ì˜ ê³µë°± ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "\n",
    "# âœ… ëŒ€í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ì—­í•  ê¸°ë°˜)\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# âœ… ì²´ì¸ì„ ì—°ê²° (í”„ë¡¬í”„íŠ¸ â†’ ëª¨ë¸ â†’ íŒŒì„œ)\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "# âœ… ì‹¤í–‰ (í”„ë¡¬í”„íŠ¸ì— ê°’ ë„£ê³  ì²´ì¸ ì‘ë™)\n",
    "response = chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"ìš°ì£¼í–‰ì„±ì—ëŠ” ì–´ë–¤ê²ƒì´ìˆì§€?\"\n",
    "})\n",
    "\n",
    "# âœ… ê²°ê³¼ ì¶œë ¥\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05258a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['íŠ¸ë¤¼í”Œ', 'ì¹´ë¹„ì–´', 'ì‚¬í”„ë€', 'ì™€ê·œ', 'ë¸”ë™ ê°€ë¦¬ë¹„']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "# âœ… ì¶œë ¥ íŒŒì„œë¥¼ ë§Œë“¤ì–´ ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))  # ê° í•­ëª©ì˜ ê³µë°± ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "\n",
    "# âœ… ëŒ€í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ì—­í•  ê¸°ë°˜)â†’ AIì—ê²Œ ì—­í• ì„ ì¤˜ìš”.\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "\n",
    "# âœ… ì²´ì¸ì„ ì—°ê²° (í”„ë¡¬í”„íŠ¸ â†’ ëª¨ë¸ â†’ íŒŒì„œ) \n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"ì„¸ìƒì—ì„œ ê°€ì¥ ë¹„ì‹¼ ìŒì‹ì€?\"\n",
    "})\n",
    "\n",
    "# âœ… ê²°ê³¼ ì¶œë ¥\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db4470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
